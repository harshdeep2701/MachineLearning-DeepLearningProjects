{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuINnXCuM904",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuAQU1tGNFZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "movies = pd.read_csv('movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users = pd.read_csv('users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings = pd.read_csv('ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJQFmKYmNLyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the training set and the test set\n",
        "training_set = pd.read_csv('training_set.csv')\n",
        "training_set = np.array(training_set, dtype = 'int')\n",
        "test_set = pd.read_csv('test_set.csv')\n",
        "test_set = np.array(test_set, dtype = 'int')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_6mpILTNO6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting the number of users and movies\n",
        "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUUx4H1cNSH4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the data into an array with users in lines and movies in columns\n",
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_users in range(1, nb_users + 1):\n",
        "        id_movies = data[:,1][data[:,0] == id_users]\n",
        "        id_ratings = data[:,2][data[:,0] == id_users]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies - 1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data\n",
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)\n",
        "\n",
        "# Converting the data into Torch tensors\n",
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYii82cjNWJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the architecture of the Neural Network\n",
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGLR8AbMNfOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "609e0a64-1dc9-4328-d3d9-407cd3afd0a2"
      },
      "source": [
        "# Training the SAE\n",
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch + 1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.item()*mean_corrector)\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 1.347552208116566\n",
            "epoch: 2 loss: 1.0100239018306674\n",
            "epoch: 3 loss: 0.9899552653401554\n",
            "epoch: 4 loss: 0.983409733168408\n",
            "epoch: 5 loss: 0.9801102859794218\n",
            "epoch: 6 loss: 0.9783306696283256\n",
            "epoch: 7 loss: 0.9772230760206483\n",
            "epoch: 8 loss: 0.9765539533840917\n",
            "epoch: 9 loss: 0.9759521024773594\n",
            "epoch: 10 loss: 0.9755646719100741\n",
            "epoch: 11 loss: 0.9752691750719578\n",
            "epoch: 12 loss: 0.9750754809559671\n",
            "epoch: 13 loss: 0.9747863156013553\n",
            "epoch: 14 loss: 0.974521692563073\n",
            "epoch: 15 loss: 0.9743868812897604\n",
            "epoch: 16 loss: 0.9743234041433538\n",
            "epoch: 17 loss: 0.9742060630349182\n",
            "epoch: 18 loss: 0.9740831721721069\n",
            "epoch: 19 loss: 0.9741085321901034\n",
            "epoch: 20 loss: 0.9738770455462931\n",
            "epoch: 21 loss: 0.9737450500542527\n",
            "epoch: 22 loss: 0.9736409504787895\n",
            "epoch: 23 loss: 0.9734747169499968\n",
            "epoch: 24 loss: 0.9733123110640378\n",
            "epoch: 25 loss: 0.9726667238259846\n",
            "epoch: 26 loss: 0.9720728098820459\n",
            "epoch: 27 loss: 0.9713710876281774\n",
            "epoch: 28 loss: 0.9704779337068687\n",
            "epoch: 29 loss: 0.9704903095565774\n",
            "epoch: 30 loss: 0.9690685620997631\n",
            "epoch: 31 loss: 0.9683842461592148\n",
            "epoch: 32 loss: 0.9672612827397259\n",
            "epoch: 33 loss: 0.9663425843034086\n",
            "epoch: 34 loss: 0.9665148549225129\n",
            "epoch: 35 loss: 0.965665867026159\n",
            "epoch: 36 loss: 0.965678787984071\n",
            "epoch: 37 loss: 0.966173653569692\n",
            "epoch: 38 loss: 0.9643521159477871\n",
            "epoch: 39 loss: 0.9631774579774324\n",
            "epoch: 40 loss: 0.9635386400107279\n",
            "epoch: 41 loss: 0.9656435568180303\n",
            "epoch: 42 loss: 0.9626453974609197\n",
            "epoch: 43 loss: 0.9603456878091757\n",
            "epoch: 44 loss: 0.9609268925106996\n",
            "epoch: 45 loss: 0.9587833147822193\n",
            "epoch: 46 loss: 0.9569044276951749\n",
            "epoch: 47 loss: 0.9570242242269515\n",
            "epoch: 48 loss: 0.9550794538471468\n",
            "epoch: 49 loss: 0.9536623056586186\n",
            "epoch: 50 loss: 0.9523446778648533\n",
            "epoch: 51 loss: 0.9554250055582996\n",
            "epoch: 52 loss: 0.9568697485095802\n",
            "epoch: 53 loss: 0.9539279201816875\n",
            "epoch: 54 loss: 0.9538244688776469\n",
            "epoch: 55 loss: 0.9511701184902343\n",
            "epoch: 56 loss: 0.9507895691660863\n",
            "epoch: 57 loss: 0.9491108269405615\n",
            "epoch: 58 loss: 0.9470834424678604\n",
            "epoch: 59 loss: 0.9470552880343666\n",
            "epoch: 60 loss: 0.9473406625322539\n",
            "epoch: 61 loss: 0.9447083750586076\n",
            "epoch: 62 loss: 0.9430426439233661\n",
            "epoch: 63 loss: 0.9415085635223587\n",
            "epoch: 64 loss: 0.9411537167794521\n",
            "epoch: 65 loss: 0.940380514082025\n",
            "epoch: 66 loss: 0.9389949107389599\n",
            "epoch: 67 loss: 0.9402125018590348\n",
            "epoch: 68 loss: 0.9383328563834816\n",
            "epoch: 69 loss: 0.936878666878117\n",
            "epoch: 70 loss: 0.9346753759674348\n",
            "epoch: 71 loss: 0.9368443518882525\n",
            "epoch: 72 loss: 0.9363180668463332\n",
            "epoch: 73 loss: 0.9357222955290959\n",
            "epoch: 74 loss: 0.937672575983387\n",
            "epoch: 75 loss: 0.9383145976700505\n",
            "epoch: 76 loss: 0.9348457037320345\n",
            "epoch: 77 loss: 0.9326070500032819\n",
            "epoch: 78 loss: 0.9309447550810235\n",
            "epoch: 79 loss: 0.9310132860489196\n",
            "epoch: 80 loss: 0.9337955944283542\n",
            "epoch: 81 loss: 0.9289631541789326\n",
            "epoch: 82 loss: 0.9263929313483948\n",
            "epoch: 83 loss: 0.9252162663285021\n",
            "epoch: 84 loss: 0.9241138882628278\n",
            "epoch: 85 loss: 0.9252648447351536\n",
            "epoch: 86 loss: 0.9259746388397178\n",
            "epoch: 87 loss: 0.9315765729522096\n",
            "epoch: 88 loss: 0.9293174050416638\n",
            "epoch: 89 loss: 0.92466098667443\n",
            "epoch: 90 loss: 0.9276912467137479\n",
            "epoch: 91 loss: 0.9286004359190049\n",
            "epoch: 92 loss: 0.9232674238063832\n",
            "epoch: 93 loss: 0.9214634220059446\n",
            "epoch: 94 loss: 0.9192544416563703\n",
            "epoch: 95 loss: 0.9178829947242382\n",
            "epoch: 96 loss: 0.9162722831411914\n",
            "epoch: 97 loss: 0.9156684043080046\n",
            "epoch: 98 loss: 0.9164565109238201\n",
            "epoch: 99 loss: 0.9142189907610947\n",
            "epoch: 100 loss: 0.9147956612046715\n",
            "epoch: 101 loss: 0.9136132365079375\n",
            "epoch: 102 loss: 0.9117422634868589\n",
            "epoch: 103 loss: 0.9120247760744423\n",
            "epoch: 104 loss: 0.9111505608808587\n",
            "epoch: 105 loss: 0.9101180895267605\n",
            "epoch: 106 loss: 0.9115856659887166\n",
            "epoch: 107 loss: 0.9100716987160169\n",
            "epoch: 108 loss: 0.908549650324344\n",
            "epoch: 109 loss: 0.9084116940242208\n",
            "epoch: 110 loss: 0.9109282210776285\n",
            "epoch: 111 loss: 0.9092199312927118\n",
            "epoch: 112 loss: 0.9065673098433049\n",
            "epoch: 113 loss: 0.9072656202233752\n",
            "epoch: 114 loss: 0.908667847463665\n",
            "epoch: 115 loss: 0.9066905156158275\n",
            "epoch: 116 loss: 0.9070435042968384\n",
            "epoch: 117 loss: 0.9054101694364822\n",
            "epoch: 118 loss: 0.904633095113543\n",
            "epoch: 119 loss: 0.9060690945462904\n",
            "epoch: 120 loss: 0.9051666128711465\n",
            "epoch: 121 loss: 0.9065098633482498\n",
            "epoch: 122 loss: 0.9146909951112216\n",
            "epoch: 123 loss: 0.9130092311364287\n",
            "epoch: 124 loss: 0.9110716046561161\n",
            "epoch: 125 loss: 0.9100650637964236\n",
            "epoch: 126 loss: 0.9093829239262609\n",
            "epoch: 127 loss: 0.90816101290818\n",
            "epoch: 128 loss: 0.9081319387797541\n",
            "epoch: 129 loss: 0.907298809104\n",
            "epoch: 130 loss: 0.9066140602727074\n",
            "epoch: 131 loss: 0.9071570196525931\n",
            "epoch: 132 loss: 0.9060592332473748\n",
            "epoch: 133 loss: 0.9054889134607119\n",
            "epoch: 134 loss: 0.9060261612293945\n",
            "epoch: 135 loss: 0.90663239568438\n",
            "epoch: 136 loss: 0.9061455392000457\n",
            "epoch: 137 loss: 0.9049581596938784\n",
            "epoch: 138 loss: 0.9055542286517617\n",
            "epoch: 139 loss: 0.9053576598948806\n",
            "epoch: 140 loss: 0.9042592440723476\n",
            "epoch: 141 loss: 0.9036915327034738\n",
            "epoch: 142 loss: 0.9034624298841287\n",
            "epoch: 143 loss: 0.9030104060666877\n",
            "epoch: 144 loss: 0.9031907248331096\n",
            "epoch: 145 loss: 0.9023544878588942\n",
            "epoch: 146 loss: 0.902429558047349\n",
            "epoch: 147 loss: 0.9024730534950692\n",
            "epoch: 148 loss: 0.903502056294851\n",
            "epoch: 149 loss: 0.903341683335703\n",
            "epoch: 150 loss: 0.9024851498278758\n",
            "epoch: 151 loss: 0.9021242672242454\n",
            "epoch: 152 loss: 0.9020305545237255\n",
            "epoch: 153 loss: 0.9015107014427995\n",
            "epoch: 154 loss: 0.9011666728458271\n",
            "epoch: 155 loss: 0.9009289225885819\n",
            "epoch: 156 loss: 0.9006320798163441\n",
            "epoch: 157 loss: 0.9004294895750826\n",
            "epoch: 158 loss: 0.9002290904937403\n",
            "epoch: 159 loss: 0.9005356721990757\n",
            "epoch: 160 loss: 0.9116819871792878\n",
            "epoch: 161 loss: 0.9101879815445406\n",
            "epoch: 162 loss: 0.9025590697491293\n",
            "epoch: 163 loss: 0.9013133299484263\n",
            "epoch: 164 loss: 0.901938837910413\n",
            "epoch: 165 loss: 0.9013133262358418\n",
            "epoch: 166 loss: 0.8999293859335786\n",
            "epoch: 167 loss: 0.900189839541083\n",
            "epoch: 168 loss: 0.8995830224011343\n",
            "epoch: 169 loss: 0.8997212159589042\n",
            "epoch: 170 loss: 0.9023117922923685\n",
            "epoch: 171 loss: 0.9028902529251511\n",
            "epoch: 172 loss: 0.9031824612530337\n",
            "epoch: 173 loss: 0.9026955240730269\n",
            "epoch: 174 loss: 0.9017958557748769\n",
            "epoch: 175 loss: 0.9032525658092295\n",
            "epoch: 176 loss: 0.9033172001094507\n",
            "epoch: 177 loss: 0.9040674881690329\n",
            "epoch: 178 loss: 0.9073302012266128\n",
            "epoch: 179 loss: 0.9047983032249538\n",
            "epoch: 180 loss: 0.9043311005374285\n",
            "epoch: 181 loss: 0.9046771247493693\n",
            "epoch: 182 loss: 0.9087616028557968\n",
            "epoch: 183 loss: 0.9046044125480056\n",
            "epoch: 184 loss: 0.9027768204032199\n",
            "epoch: 185 loss: 0.9045447641575124\n",
            "epoch: 186 loss: 0.9026348878857295\n",
            "epoch: 187 loss: 0.9019011037811778\n",
            "epoch: 188 loss: 0.9022897580748905\n",
            "epoch: 189 loss: 0.9015947771345603\n",
            "epoch: 190 loss: 0.9001956211159781\n",
            "epoch: 191 loss: 0.9005687306310579\n",
            "epoch: 192 loss: 0.8997544080024957\n",
            "epoch: 193 loss: 0.9017388416732002\n",
            "epoch: 194 loss: 0.9008283453848002\n",
            "epoch: 195 loss: 0.8993969211122578\n",
            "epoch: 196 loss: 0.8991850234149162\n",
            "epoch: 197 loss: 0.8994896368713535\n",
            "epoch: 198 loss: 0.8988567230367246\n",
            "epoch: 199 loss: 0.8978279984378841\n",
            "epoch: 200 loss: 0.8985160321891346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSSAiyPKNi1O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c0b0435-4969-4eb3-c994-e7cd4c656600"
      },
      "source": [
        "# Testing the SAE\n",
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae(input)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
        "        test_loss += np.sqrt(loss.item()*mean_corrector)\n",
        "        s += 1.\n",
        "print('test loss: '+str(test_loss/s))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test loss: 0.9212857495993412\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}